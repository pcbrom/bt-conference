{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               abstract  Repetition  \\\n",
      "0     文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...           1   \n",
      "1     文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...           2   \n",
      "2     文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...           3   \n",
      "3     “天然药物化学”是高等学校药学及相关专业的必修课程。课程章节内容多、理论性强,学生学习面临较...           1   \n",
      "4     “天然药物化学”是高等学校药学及相关专业的必修课程。课程章节内容多、理论性强,学生学习面临较...           2   \n",
      "...                                                 ...         ...   \n",
      "1065  绿色化学分析技术，即最大限度地减少或者避免有害化学品被应用于分析过程当中，从而实现环境保护与...           1   \n",
      "1066  绿色化学分析技术，即最大限度地减少或者避免有害化学品被应用于分析过程当中，从而实现环境保护与...           2   \n",
      "1067  绿色化学分析技术，即最大限度地减少或者避免有害化学品被应用于分析过程当中，从而实现环境保护与...           3   \n",
      "1068  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...           1   \n",
      "1069  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...           2   \n",
      "\n",
      "                                                  ZH_EN  \\\n",
      "0     The article first elaborates on the connection...   \n",
      "1     The article first elaborates on the relationsh...   \n",
      "2     The article first elucidates the relationship ...   \n",
      "3     Natural Product Chemistry is a required course...   \n",
      "4     \"Natural Products Chemistry\" is a required cou...   \n",
      "...                                                 ...   \n",
      "1065  Green analytical chemistry technology, which a...   \n",
      "1066  Green chemistry analytical technology aims to ...   \n",
      "1067  Green chemistry analysis technology aims to mi...   \n",
      "1068  The article first expounds on the connection b...   \n",
      "1069  The article first explains the connection betw...   \n",
      "\n",
      "                                                  EN_ZH              model  \n",
      "0     文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的联系。接着，探讨了在工程教育专...        Gemini-2FTE  \n",
      "1     文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的关系。其次，探讨了在工程教育专...        Gemini-2FTE  \n",
      "2     文章首先阐述工程教育专业认证与应用化学生产实习课程的关系。然后探讨工程教育专业认证背景下应用...        Gemini-2FTE  \n",
      "3     天然产物化学是高等院校药学及相关专业的必修课。课程内容章节丰富且理论性强，学生在学习中面临诸...        Gemini-2FTE  \n",
      "4     天然药物化学是高校药学及相关专业的必修课。该课程内容广泛且理论性强，对学生的学习造成挑战。因...        Gemini-2FTE  \n",
      "...                                                 ...                ...  \n",
      "1065  绿色分析化学技术旨在最小化或避免在分析过程中使用有害化学品，从而实现环境保护和可持续发展。文...          Grok-Beta  \n",
      "1066  绿色化学分析技术旨在减少或避免在分析过程中使用有害化学品，从而实现环境保护和可持续发展。文章...          Grok-Beta  \n",
      "1067  绿色化学分析技术旨在减少或避免在分析过程中使用有害化学品，从而促进环境保护和可持续发展。文章...          Grok-Beta  \n",
      "1068  本文首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的联系。然后讨论了工程教育专业认...  Claude-Sonnet-3.7  \n",
      "1069  本文首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的联系，然后讨论了工程教育专业认...  Claude-Sonnet-3.7  \n",
      "\n",
      "[1070 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "\n",
    "df_gemini = pd.read_csv(\"results_data/experimental_design_results_gemini-2.0-flash-thinking-exp_reprocessed.csv\")\n",
    "df_gemini['model'] = 'Gemini-2FTE' # gemini-2.0-flash-thinking-exp\n",
    "\n",
    "df_deepseek = pd.read_csv(\"results_data/experimental_design_results_deepseek-chat.csv\")\n",
    "df_deepseek['model'] = 'DeepSeek-V3' # deepseek-chat v3\n",
    "\n",
    "df_gpt = pd.read_csv(\"results_data/experimental_design_results_gpt-4.5-preview-2025-02-27.csv\")\n",
    "df_gpt['model'] = 'OpenAI-GPT-4.5' # gpt-4.5-preview-2025-02-27\n",
    "\n",
    "df_grok = pd.read_csv(\"results_data/experimental_design_results_grok-beta.csv\")\n",
    "df_grok['model'] = 'Grok-Beta' # grok-beta\n",
    "\n",
    "df_sonnet = pd.read_csv(\"results_data/experimental_design_results_claude-3-7-sonnet-20250219.csv\")\n",
    "df_sonnet['model'] = 'Claude-Sonnet-3.7' # claude-3-7-sonnet-20250219\n",
    "\n",
    "# Stack data frames\n",
    "df = pd.concat([df_gemini, df_deepseek, df_gpt, df_grok, df_sonnet], ignore_index=True)\n",
    "\n",
    "# Print\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.888 seconds.\n",
      "Loading model cost 0.890 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.897 seconds.\n",
      "Loading model cost 0.903 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.907 seconds.\n",
      "Loading model cost 0.910 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.916 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.921 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.940 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.969 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.971 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.973 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.979 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.985 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.992 seconds.\n",
      "Loading model cost 1.000 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.006 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.008 seconds.\n",
      "Loading model cost 1.020 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.087 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function for Chinese texts using Jieba\n",
    "def tokenize_chinese_jieba(text):\n",
    "    # Use Jieba to cut the Chinese text into words\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "# Function to calculate BLEU (using unigram and bigram)\n",
    "def calculate_bleu(candidate_tokens, reference_tokens):\n",
    "    # We use weights (0.5, 0.5) for unigrams and bigrams\n",
    "    try:\n",
    "        return sentence_bleu([reference_tokens], candidate_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate CHRF (character n-gram F-score)\n",
    "def calculate_chrf(candidate_tokens, reference_tokens):\n",
    "    candidate_str = \"\".join(candidate_tokens)\n",
    "    reference_str = \"\".join(reference_tokens)\n",
    "    # Calculates the ratio between the intersection and the union of the characters of the reference\n",
    "    return len(set(candidate_str) & set(reference_str)) / len(set(reference_str)) if len(set(reference_str)) > 0 else 0\n",
    "\n",
    "# Function to calculate TER using TF-IDF vectorization and mean squared error\n",
    "def calculate_ter(candidate_text, reference_text):\n",
    "    vectorizer = TfidfVectorizer() #token_pattern=r\"(?u)\\b\\w+\\b\" - Removed token pattern because it is not needed for chinese\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([candidate_text, reference_text])\n",
    "        return mean_squared_error(tfidf_matrix[0].toarray(), tfidf_matrix[1].toarray())\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate Semantic Similarity (TF-IDF + cosine)\n",
    "def calculate_semantic_similarity(original, translated):\n",
    "    vectorizer = TfidfVectorizer() #token_pattern=r\"(?u)\\b\\w+\\b\" - Removed token pattern because it is not needed for chinese\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([original, translated])\n",
    "        return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Apply metrics to each row of the DataFrame\n",
    "def calculate_metrics_for_row(row):\n",
    "    original_text = row['abstract']\n",
    "    back_translation = row['EN_ZH']\n",
    "\n",
    "    # Tokenize the texts using Jieba\n",
    "    original_tokens = tokenize_chinese_jieba(original_text)\n",
    "    translated_tokens = tokenize_chinese_jieba(back_translation)\n",
    "\n",
    "    # Calculate the metrics:\n",
    "    bleu_value = calculate_bleu(translated_tokens, original_tokens)\n",
    "    chrf_value = calculate_chrf(translated_tokens, original_tokens)\n",
    "    ter_value = calculate_ter(\"\".join(translated_tokens), \"\".join(original_tokens))\n",
    "    semantic_similarity = calculate_semantic_similarity(original_text, back_translation)\n",
    "\n",
    "    return pd.Series([bleu_value, chrf_value, ter_value, semantic_similarity])\n",
    "\n",
    "def calculate_metrics_for_df(df):\n",
    "    # tqdm.pandas(desc=\"Calculating metrics\") # Removed progress bar\n",
    "    return df.apply(calculate_metrics_for_row, axis=1)\n",
    "\n",
    "def apply_parallel(df, func, n_cores=multiprocessing.cpu_count()):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "df[['BLEU', 'CHRF', 'TER', 'Semantic Similarity']] = apply_parallel(df, calculate_metrics_for_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  model                                              EN_ZH  \\\n",
      "0           Gemini-2FTE  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的联系。接着，探讨了在工程教育专...   \n",
      "1           Gemini-2FTE  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的关系。其次，探讨了在工程教育专...   \n",
      "2           Gemini-2FTE  文章首先阐述工程教育专业认证与应用化学生产实习课程的关系。然后探讨工程教育专业认证背景下应用...   \n",
      "3           Gemini-2FTE  天然产物化学是高等院校药学及相关专业的必修课。课程内容章节丰富且理论性强，学生在学习中面临诸...   \n",
      "4           Gemini-2FTE  天然药物化学是高校药学及相关专业的必修课。该课程内容广泛且理论性强，对学生的学习造成挑战。因...   \n",
      "...                 ...                                                ...   \n",
      "1065          Grok-Beta  绿色分析化学技术旨在最小化或避免在分析过程中使用有害化学品，从而实现环境保护和可持续发展。文...   \n",
      "1066          Grok-Beta  绿色化学分析技术旨在减少或避免在分析过程中使用有害化学品，从而实现环境保护和可持续发展。文章...   \n",
      "1067          Grok-Beta  绿色化学分析技术旨在减少或避免在分析过程中使用有害化学品，从而促进环境保护和可持续发展。文章...   \n",
      "1068  Claude-Sonnet-3.7  本文首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的联系。然后讨论了工程教育专业认...   \n",
      "1069  Claude-Sonnet-3.7  本文首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的联系，然后讨论了工程教育专业认...   \n",
      "\n",
      "          BLEU      CHRF       TER  Semantic Similarity  \n",
      "0     0.693889  0.909091  0.141732             0.078745  \n",
      "1     0.694830  0.909091  0.121337             0.150640  \n",
      "2     0.698251  0.909091  0.148285             0.184432  \n",
      "3     0.413904  0.890110  0.072720             0.127360  \n",
      "4     0.427470  0.813187  0.069251             0.168983  \n",
      "...        ...       ...       ...                  ...  \n",
      "1065  0.528845  0.769841  0.062160             0.067593  \n",
      "1066  0.545975  0.777778  0.064128             0.070142  \n",
      "1067  0.526741  0.769841  0.062160             0.067593  \n",
      "1068  0.765074  0.848485  0.152570             0.084580  \n",
      "1069  0.760867  0.848485  0.152570             0.084580  \n",
      "\n",
      "[1070 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the results:\n",
    "print(df[['model', 'EN_ZH', 'BLEU', 'CHRF', 'TER', 'Semantic Similarity']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global Descriptive Statistics:\n",
      "|       |        BLEU |         CHRF |          TER |   Semantic Similarity |\n",
      "|:------|------------:|-------------:|-------------:|----------------------:|\n",
      "| count | 1070        | 1070         | 1070         |          1070         |\n",
      "| mean  |    0.563575 |    0.834908  |    0.0817545 |             0.102398  |\n",
      "| std   |    0.114443 |    0.0648063 |    0.101264  |             0.101932  |\n",
      "| min   |    0        |    0         |    0.013339  |             0         |\n",
      "| 25%   |    0.48754  |    0.797386  |    0.053176  |             0.0375273 |\n",
      "| 50%   |    0.571259 |    0.842105  |    0.0689655 |             0.0779243 |\n",
      "| 75%   |    0.641327 |    0.878788  |    0.0880202 |             0.131849  |\n",
      "| max   |    0.930949 |    0.96875   |    1         |             0.691046  |\n",
      "\n",
      "Descriptive Statistics by Model:\n",
      "|                                  |   C l a u d e - S o n n e t - 3 . 7 |   D e e p S e e k - V 3 |   G e m i n i - 2 F T E |   G r o k - B e t a |   O p e n A I - G P T - 4 . 5 |\n",
      "|:---------------------------------|------------------------------------:|------------------------:|------------------------:|--------------------:|------------------------------:|\n",
      "| ('BLEU', 'count')                |                          2          |             267         |             267         |         267         |                   267         |\n",
      "| ('BLEU', 'mean')                 |                          0.762971   |               0.603285  |               0.59971   |           0.52227   |                     0.527541  |\n",
      "| ('BLEU', 'std')                  |                          0.00297489 |               0.102282  |               0.113836  |           0.107349  |                     0.107158  |\n",
      "| ('BLEU', 'min')                  |                          0.760867   |               0.276982  |               0         |           0.222731  |                     0.257272  |\n",
      "| ('BLEU', '25%')                  |                          0.761919   |               0.527741  |               0.533788  |           0.450274  |                     0.457766  |\n",
      "| ('BLEU', '50%')                  |                          0.762971   |               0.605879  |               0.608069  |           0.53126   |                     0.531762  |\n",
      "| ('BLEU', '75%')                  |                          0.764023   |               0.678086  |               0.679704  |           0.601176  |                     0.593147  |\n",
      "| ('BLEU', 'max')                  |                          0.765074   |               0.824852  |               0.834775  |           0.790471  |                     0.930949  |\n",
      "| ('CHRF', 'count')                |                          2          |             267         |             267         |         267         |                   267         |\n",
      "| ('CHRF', 'mean')                 |                          0.848485   |               0.84943   |               0.853033  |           0.812656  |                     0.824409  |\n",
      "| ('CHRF', 'std')                  |                          0          |               0.0514739 |               0.0763403 |           0.0636615 |                     0.0565489 |\n",
      "| ('CHRF', 'min')                  |                          0.848485   |               0.688889  |               0         |           0.623762  |                     0.641509  |\n",
      "| ('CHRF', '25%')                  |                          0.848485   |               0.815459  |               0.826856  |           0.771507  |                     0.784314  |\n",
      "| ('CHRF', '50%')                  |                          0.848485   |               0.846154  |               0.863158  |           0.816794  |                     0.825688  |\n",
      "| ('CHRF', '75%')                  |                          0.848485   |               0.887049  |               0.893204  |           0.861197  |                     0.865277  |\n",
      "| ('CHRF', 'max')                  |                          0.848485   |               0.965517  |               0.96875   |           0.94375   |                     0.945455  |\n",
      "| ('TER', 'count')                 |                          2          |             267         |             267         |         267         |                   267         |\n",
      "| ('TER', 'mean')                  |                          0.15257    |               0.0808461 |               0.0807685 |           0.0824557 |                     0.0824172 |\n",
      "| ('TER', 'std')                   |                          0          |               0.101555  |               0.101454  |           0.101221  |                     0.101579  |\n",
      "| ('TER', 'min')                   |                          0.15257    |               0.014156  |               0.013339  |           0.0153177 |                     0.0163406 |\n",
      "| ('TER', '25%')                   |                          0.15257    |               0.0521687 |               0.0519979 |           0.0535443 |                     0.0538049 |\n",
      "| ('TER', '50%')                   |                          0.15257    |               0.0679428 |               0.0689655 |           0.07129   |                     0.0692514 |\n",
      "| ('TER', '75%')                   |                          0.15257    |               0.0869565 |               0.0846942 |           0.0907791 |                     0.0902579 |\n",
      "| ('TER', 'max')                   |                          0.15257    |               1         |               1         |           1         |                     1         |\n",
      "| ('Semantic Similarity', 'count') |                          2          |             267         |             267         |         267         |                   267         |\n",
      "| ('Semantic Similarity', 'mean')  |                          0.0845799  |               0.128719  |               0.114355  |           0.078798  |                     0.0878526 |\n",
      "| ('Semantic Similarity', 'std')   |                          0          |               0.118981  |               0.101953  |           0.0787699 |                     0.0968871 |\n",
      "| ('Semantic Similarity', 'min')   |                          0.0845799  |               0         |               0         |           0         |                     0         |\n",
      "| ('Semantic Similarity', '25%')   |                          0.0845799  |               0.0532538 |               0.0507053 |           0.026719  |                     0.0266958 |\n",
      "| ('Semantic Similarity', '50%')   |                          0.0845799  |               0.101123  |               0.0887031 |           0.0632448 |                     0.067443  |\n",
      "| ('Semantic Similarity', '75%')   |                          0.0845799  |               0.166443  |               0.144647  |           0.109687  |                     0.112516  |\n",
      "| ('Semantic Similarity', 'max')   |                          0.0845799  |               0.658562  |               0.573152  |           0.440904  |                     0.691046  |\n"
     ]
    }
   ],
   "source": [
    "# Global Descriptive Statistics\n",
    "global_stats = df[['BLEU', 'CHRF', 'TER', 'Semantic Similarity']].describe()\n",
    "print(\"\\nGlobal Descriptive Statistics:\")\n",
    "print(global_stats.to_markdown())  # Output as markdown for journal\n",
    "\n",
    "# Descriptive Statistics by Model\n",
    "model_stats = df.groupby('model')[['BLEU', 'CHRF', 'TER', 'Semantic Similarity']].describe().transpose()\n",
    "\n",
    "# Flatten the multi-level index for better readability in the table\n",
    "model_stats.columns = [' '.join(col).strip() for col in model_stats.columns.values]\n",
    "\n",
    "print(\"\\nDescriptive Statistics by Model:\")\n",
    "print(model_stats.to_markdown()) # Output as markdown for journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results_metrics/results_metrics.csv\", index=False)\n",
    "df.to_excel(\"results_metrics/results_metrics.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
