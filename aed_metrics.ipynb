{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              abstract  Repetition  \\\n",
      "0    文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...           1   \n",
      "1    文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...           2   \n",
      "2    文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...           3   \n",
      "3    “天然药物化学”是高等学校药学及相关专业的必修课程。课程章节内容多、理论性强,学生学习面临较...           1   \n",
      "4    “天然药物化学”是高等学校药学及相关专业的必修课程。课程章节内容多、理论性强,学生学习面临较...           2   \n",
      "..                                                 ...         ...   \n",
      "796  <正>化学作为一门自然科学，是人们认识世界和改造世界的重要途径。在历史长河中，化学是经由无数...           2   \n",
      "797  <正>化学作为一门自然科学，是人们认识世界和改造世界的重要途径。在历史长河中，化学是经由无数...           3   \n",
      "798  绿色化学分析技术，即最大限度地减少或者避免有害化学品被应用于分析过程当中，从而实现环境保护与...           1   \n",
      "799  绿色化学分析技术，即最大限度地减少或者避免有害化学品被应用于分析过程当中，从而实现环境保护与...           2   \n",
      "800  绿色化学分析技术，即最大限度地减少或者避免有害化学品被应用于分析过程当中，从而实现环境保护与...           3   \n",
      "\n",
      "                                                 ZH_EN  \\\n",
      "0    The article first elaborates on the connection...   \n",
      "1    The article first elaborates on the relationsh...   \n",
      "2    The article first elucidates the relationship ...   \n",
      "3    Natural Product Chemistry is a required course...   \n",
      "4    \"Natural Products Chemistry\" is a required cou...   \n",
      "..                                                 ...   \n",
      "796  Chemistry, as a natural science, is an importa...   \n",
      "797  Chemistry, as a natural science, is an essenti...   \n",
      "798  Green analytical chemistry refers to minimizin...   \n",
      "799  Green analytical chemistry refers to minimizin...   \n",
      "800  Green analytical chemistry technology refers t...   \n",
      "\n",
      "                                                 EN_ZH    model  \n",
      "0    文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的联系。接着，探讨了在工程教育专...   G-2FTE  \n",
      "1    文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的关系。其次，探讨了在工程教育专...   G-2FTE  \n",
      "2    文章首先阐述工程教育专业认证与应用化学生产实习课程的关系。然后探讨工程教育专业认证背景下应用...   G-2FTE  \n",
      "3    天然产物化学是高等院校药学及相关专业的必修课。课程内容章节丰富且理论性强，学生在学习中面临诸...   G-2FTE  \n",
      "4    天然药物化学是高校药学及相关专业的必修课。该课程内容广泛且理论性强，对学生的学习造成挑战。因...   G-2FTE  \n",
      "..                                                 ...      ...  \n",
      "796  化学作为一门自然科学，是人类认识和改造世界的重要手段。在历史长河中，化学经历了无数次实验和探...  GPT-4.5  \n",
      "797  化学作为一种自然科学，是人类认识和改造世界的重要途径。在历史长河中，化学通过无数次实验研究逐...  GPT-4.5  \n",
      "798  绿色分析化学是指在分析过程中尽量减少或避免使用有害化学品，从而实现环境保护和可持续发展。文章...  GPT-4.5  \n",
      "799  绿色分析化学是指在分析过程中尽量减少或避免使用有害化学物质，以实现环境保护和可持续发展的目标...  GPT-4.5  \n",
      "800  绿色分析化学技术是指在分析过程中尽量减少或消除有害化学品的使用，从而实现环境保护和可持续发展...  GPT-4.5  \n",
      "\n",
      "[801 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "\n",
    "df_gemini = pd.read_csv(\"results_data/experimental_design_results_gemini-2.0-flash-thinking-exp_reprocessed.csv\")\n",
    "df_gemini['model'] = 'G-2FTE' # gemini-2.0-flash-thinking-exp\n",
    "\n",
    "df_deepseek = pd.read_csv(\"results_data/experimental_design_results_deepseek-chat.csv\")\n",
    "df_deepseek['model'] = 'DS-V3' # deepseek-chat v3\n",
    "\n",
    "df_gpt = pd.read_csv(\"results_data/experimental_design_results_gpt-4.5-preview-2025-02-27.csv\")\n",
    "df_gpt['model'] = 'GPT-4.5' # gpt-4.5-preview-2025-02-27\n",
    "\n",
    "# Stack data frames\n",
    "df = pd.concat([df_gemini, df_deepseek, df_gpt], ignore_index=True)\n",
    "\n",
    "# Print\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.027 seconds.\n",
      "Loading model cost 1.017 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.030 seconds.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Prefix dict has been built successfully.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.047 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.044 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.053 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.090 seconds.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Prefix dict has been built successfully.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.106 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.113 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.127 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.107 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.132 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.148 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.157 seconds.\n",
      "Loading model cost 1.172 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.173 seconds.\n",
      "Loading model cost 1.171 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.190 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.275 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.356 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function for Chinese texts using Jieba\n",
    "def tokenize_chinese_jieba(text):\n",
    "    # Use Jieba to cut the Chinese text into words\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "# Function to calculate BLEU (using unigram and bigram)\n",
    "def calculate_bleu(candidate_tokens, reference_tokens):\n",
    "    # We use weights (0.5, 0.5) for unigrams and bigrams\n",
    "    try:\n",
    "        return sentence_bleu([reference_tokens], candidate_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate CHRF (character n-gram F-score)\n",
    "def calculate_chrf(candidate_tokens, reference_tokens):\n",
    "    candidate_str = \"\".join(candidate_tokens)\n",
    "    reference_str = \"\".join(reference_tokens)\n",
    "    # Calculates the ratio between the intersection and the union of the characters of the reference\n",
    "    return len(set(candidate_str) & set(reference_str)) / len(set(reference_str)) if len(set(reference_str)) > 0 else 0\n",
    "\n",
    "# Function to calculate TER using TF-IDF vectorization and mean squared error\n",
    "def calculate_ter(candidate_text, reference_text):\n",
    "    vectorizer = TfidfVectorizer() #token_pattern=r\"(?u)\\b\\w+\\b\" - Removed token pattern because it is not needed for chinese\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([candidate_text, reference_text])\n",
    "        return mean_squared_error(tfidf_matrix[0].toarray(), tfidf_matrix[1].toarray())\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate Semantic Similarity (TF-IDF + cosine)\n",
    "def calculate_semantic_similarity(original, translated):\n",
    "    vectorizer = TfidfVectorizer() #token_pattern=r\"(?u)\\b\\w+\\b\" - Removed token pattern because it is not needed for chinese\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([original, translated])\n",
    "        return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Apply metrics to each row of the DataFrame\n",
    "def calculate_metrics_for_row(row):\n",
    "    original_text = row['abstract']\n",
    "    back_translation = row['EN_ZH']\n",
    "\n",
    "    # Tokenize the texts using Jieba\n",
    "    original_tokens = tokenize_chinese_jieba(original_text)\n",
    "    translated_tokens = tokenize_chinese_jieba(back_translation)\n",
    "\n",
    "    # Calculate the metrics:\n",
    "    bleu_value = calculate_bleu(translated_tokens, original_tokens)\n",
    "    chrf_value = calculate_chrf(translated_tokens, original_tokens)\n",
    "    ter_value = calculate_ter(\"\".join(translated_tokens), \"\".join(original_tokens))\n",
    "    semantic_similarity = calculate_semantic_similarity(original_text, back_translation)\n",
    "\n",
    "    return pd.Series([bleu_value, chrf_value, ter_value, semantic_similarity])\n",
    "\n",
    "def calculate_metrics_for_df(df):\n",
    "    # tqdm.pandas(desc=\"Calculating metrics\") # Removed progress bar\n",
    "    return df.apply(calculate_metrics_for_row, axis=1)\n",
    "\n",
    "def apply_parallel(df, func, n_cores=multiprocessing.cpu_count()):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "df[['BLEU', 'CHRF', 'TER', 'Semantic Similarity']] = apply_parallel(df, calculate_metrics_for_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       model                                              EN_ZH      BLEU  \\\n",
      "0     G-2FTE  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的联系。接着，探讨了在工程教育专...  0.693889   \n",
      "1     G-2FTE  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的关系。其次，探讨了在工程教育专...  0.694830   \n",
      "2     G-2FTE  文章首先阐述工程教育专业认证与应用化学生产实习课程的关系。然后探讨工程教育专业认证背景下应用...  0.698251   \n",
      "3     G-2FTE  天然产物化学是高等院校药学及相关专业的必修课。课程内容章节丰富且理论性强，学生在学习中面临诸...  0.413904   \n",
      "4     G-2FTE  天然药物化学是高校药学及相关专业的必修课。该课程内容广泛且理论性强，对学生的学习造成挑战。因...  0.427470   \n",
      "..       ...                                                ...       ...   \n",
      "796  GPT-4.5  化学作为一门自然科学，是人类认识和改造世界的重要手段。在历史长河中，化学经历了无数次实验和探...  0.484526   \n",
      "797  GPT-4.5  化学作为一种自然科学，是人类认识和改造世界的重要途径。在历史长河中，化学通过无数次实验研究逐...  0.452457   \n",
      "798  GPT-4.5  绿色分析化学是指在分析过程中尽量减少或避免使用有害化学品，从而实现环境保护和可持续发展。文章...  0.575077   \n",
      "799  GPT-4.5  绿色分析化学是指在分析过程中尽量减少或避免使用有害化学物质，以实现环境保护和可持续发展的目标...  0.524631   \n",
      "800  GPT-4.5  绿色分析化学技术是指在分析过程中尽量减少或消除有害化学品的使用，从而实现环境保护和可持续发展...  0.591589   \n",
      "\n",
      "         CHRF       TER  Semantic Similarity  \n",
      "0    0.909091  0.141732             0.078745  \n",
      "1    0.909091  0.121337             0.150640  \n",
      "2    0.909091  0.148285             0.184432  \n",
      "3    0.890110  0.072720             0.127360  \n",
      "4    0.813187  0.069251             0.168983  \n",
      "..        ...       ...                  ...  \n",
      "796  0.750000  0.062160             0.067593  \n",
      "797  0.750000  0.064406             0.033905  \n",
      "798  0.793651  0.061734             0.104854  \n",
      "799  0.817460  0.064128             0.070142  \n",
      "800  0.793651  0.059921             0.101179  \n",
      "\n",
      "[801 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the results:\n",
    "print(df[['model', 'EN_ZH', 'BLEU', 'CHRF', 'TER', 'Semantic Similarity']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global Descriptive Statistics:\n",
      "|       |       BLEU |        CHRF |         TER |   Semantic Similarity |\n",
      "|:------|-----------:|------------:|------------:|----------------------:|\n",
      "| count | 801        | 801         | 801         |           801         |\n",
      "| mean  |   0.576845 |   0.842291  |   0.0813439 |             0.110309  |\n",
      "| std   |   0.113245 |   0.0635943 |   0.101405  |             0.10757   |\n",
      "| min   |   0        |   0         |   0.013339  |             0         |\n",
      "| 25%   |   0.504823 |   0.809524  |   0.0527516 |             0.0396936 |\n",
      "| 50%   |   0.581845 |   0.846774  |   0.0688452 |             0.0842863 |\n",
      "| 75%   |   0.655603 |   0.884211  |   0.0869565 |             0.144384  |\n",
      "| max   |   0.930949 |   0.96875   |   1         |             0.691046  |\n",
      "\n",
      "Descriptive Statistics by Model:\n",
      "|                                  |   D S - V 3 |   G - 2 F T E |   G P T - 4 . 5 |\n",
      "|:---------------------------------|------------:|--------------:|----------------:|\n",
      "| ('BLEU', 'count')                | 267         |   267         |     267         |\n",
      "| ('BLEU', 'mean')                 |   0.603285  |     0.59971   |       0.527541  |\n",
      "| ('BLEU', 'std')                  |   0.102282  |     0.113836  |       0.107158  |\n",
      "| ('BLEU', 'min')                  |   0.276982  |     0         |       0.257272  |\n",
      "| ('BLEU', '25%')                  |   0.527741  |     0.533788  |       0.457766  |\n",
      "| ('BLEU', '50%')                  |   0.605879  |     0.608069  |       0.531762  |\n",
      "| ('BLEU', '75%')                  |   0.678086  |     0.679704  |       0.593147  |\n",
      "| ('BLEU', 'max')                  |   0.824852  |     0.834775  |       0.930949  |\n",
      "| ('CHRF', 'count')                | 267         |   267         |     267         |\n",
      "| ('CHRF', 'mean')                 |   0.84943   |     0.853033  |       0.824409  |\n",
      "| ('CHRF', 'std')                  |   0.0514739 |     0.0763403 |       0.0565489 |\n",
      "| ('CHRF', 'min')                  |   0.688889  |     0         |       0.641509  |\n",
      "| ('CHRF', '25%')                  |   0.815459  |     0.826856  |       0.784314  |\n",
      "| ('CHRF', '50%')                  |   0.846154  |     0.863158  |       0.825688  |\n",
      "| ('CHRF', '75%')                  |   0.887049  |     0.893204  |       0.865277  |\n",
      "| ('CHRF', 'max')                  |   0.965517  |     0.96875   |       0.945455  |\n",
      "| ('TER', 'count')                 | 267         |   267         |     267         |\n",
      "| ('TER', 'mean')                  |   0.0808461 |     0.0807685 |       0.0824172 |\n",
      "| ('TER', 'std')                   |   0.101555  |     0.101454  |       0.101579  |\n",
      "| ('TER', 'min')                   |   0.014156  |     0.013339  |       0.0163406 |\n",
      "| ('TER', '25%')                   |   0.0521687 |     0.0519979 |       0.0538049 |\n",
      "| ('TER', '50%')                   |   0.0679428 |     0.0689655 |       0.0692514 |\n",
      "| ('TER', '75%')                   |   0.0869565 |     0.0846942 |       0.0902579 |\n",
      "| ('TER', 'max')                   |   1         |     1         |       1         |\n",
      "| ('Semantic Similarity', 'count') | 267         |   267         |     267         |\n",
      "| ('Semantic Similarity', 'mean')  |   0.128719  |     0.114355  |       0.0878526 |\n",
      "| ('Semantic Similarity', 'std')   |   0.118981  |     0.101953  |       0.0968871 |\n",
      "| ('Semantic Similarity', 'min')   |   0         |     0         |       0         |\n",
      "| ('Semantic Similarity', '25%')   |   0.0532538 |     0.0507053 |       0.0266958 |\n",
      "| ('Semantic Similarity', '50%')   |   0.101123  |     0.0887031 |       0.067443  |\n",
      "| ('Semantic Similarity', '75%')   |   0.166443  |     0.144647  |       0.112516  |\n",
      "| ('Semantic Similarity', 'max')   |   0.658562  |     0.573152  |       0.691046  |\n"
     ]
    }
   ],
   "source": [
    "# Global Descriptive Statistics\n",
    "global_stats = df[['BLEU', 'CHRF', 'TER', 'Semantic Similarity']].describe()\n",
    "print(\"\\nGlobal Descriptive Statistics:\")\n",
    "print(global_stats.to_markdown())  # Output as markdown for journal\n",
    "\n",
    "# Descriptive Statistics by Model\n",
    "model_stats = df.groupby('model')[['BLEU', 'CHRF', 'TER', 'Semantic Similarity']].describe().transpose()\n",
    "\n",
    "# Flatten the multi-level index for better readability in the table\n",
    "model_stats.columns = [' '.join(col).strip() for col in model_stats.columns.values]\n",
    "\n",
    "print(\"\\nDescriptive Statistics by Model:\")\n",
    "print(model_stats.to_markdown()) # Output as markdown for journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results_metrics/results_metrics.csv\", index=False)\n",
    "df.to_excel(\"results_metrics/results_metrics.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
