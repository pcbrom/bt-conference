{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               abstract  Repetition  \\\n",
      "0     文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...           1   \n",
      "1     文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...           2   \n",
      "2     文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...           3   \n",
      "3     “天然药物化学”是高等学校药学及相关专业的必修课程。课程章节内容多、理论性强,学生学习面临较...           1   \n",
      "4     “天然药物化学”是高等学校药学及相关专业的必修课程。课程章节内容多、理论性强,学生学习面临较...           2   \n",
      "...                                                 ...         ...   \n",
      "1330  <正>化学作为一门自然科学，是人们认识世界和改造世界的重要途径。在历史长河中，化学是经由无数...           2   \n",
      "1331  <正>化学作为一门自然科学，是人们认识世界和改造世界的重要途径。在历史长河中，化学是经由无数...           3   \n",
      "1332  绿色化学分析技术，即最大限度地减少或者避免有害化学品被应用于分析过程当中，从而实现环境保护与...           1   \n",
      "1333  绿色化学分析技术，即最大限度地减少或者避免有害化学品被应用于分析过程当中，从而实现环境保护与...           2   \n",
      "1334  绿色化学分析技术，即最大限度地减少或者避免有害化学品被应用于分析过程当中，从而实现环境保护与...           3   \n",
      "\n",
      "                                                  ZH_EN  \\\n",
      "0     The article first elaborates on the connection...   \n",
      "1     The article first elaborates on the relationsh...   \n",
      "2     The article first elucidates the relationship ...   \n",
      "3     Natural Product Chemistry is a required course...   \n",
      "4     \"Natural Products Chemistry\" is a required cou...   \n",
      "...                                                 ...   \n",
      "1330  Chemistry, as a natural science, is an importa...   \n",
      "1331  Chemistry, as a natural science, is an importa...   \n",
      "1332  Green chemical analytical technology refers to...   \n",
      "1333  Green chemistry analytical technology aims to ...   \n",
      "1334  Green chemistry analytical technology aims to ...   \n",
      "\n",
      "                                                  EN_ZH              model  \n",
      "0     文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的联系。接着，探讨了在工程教育专...        Gemini-2FTE  \n",
      "1     文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的关系。其次，探讨了在工程教育专...        Gemini-2FTE  \n",
      "2     文章首先阐述工程教育专业认证与应用化学生产实习课程的关系。然后探讨工程教育专业认证背景下应用...        Gemini-2FTE  \n",
      "3     天然产物化学是高等院校药学及相关专业的必修课。课程内容章节丰富且理论性强，学生在学习中面临诸...        Gemini-2FTE  \n",
      "4     天然药物化学是高校药学及相关专业的必修课。该课程内容广泛且理论性强，对学生的学习造成挑战。因...        Gemini-2FTE  \n",
      "...                                                 ...                ...  \n",
      "1330  化学作为一门自然科学，是人们认识和改造世界的重要方式。纵观历史进程，化学通过无数次实验探索逐...  Claude-Sonnet-3.7  \n",
      "1331  化学作为一门自然科学，是人们认识和改造世界的重要方式。纵观历史进程，化学是通过无数次实验探索...  Claude-Sonnet-3.7  \n",
      "1332  绿色化学分析技术指的是在分析过程中最小化或避免使用有害化学物质，从而实现环境保护和可持续发展...  Claude-Sonnet-3.7  \n",
      "1333  绿色化学分析技术旨在最小化或避免在分析过程中使用有害化学品，从而实现环境保护和可持续发展。文...  Claude-Sonnet-3.7  \n",
      "1334  绿色化学分析技术旨在最小化或避免在分析过程中使用有害化学品，从而实现环境保护和可持续发展。文...  Claude-Sonnet-3.7  \n",
      "\n",
      "[1335 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "\n",
    "df_gemini = pd.read_csv(\"results_data/experimental_design_results_gemini-2.0-flash-thinking-exp_reprocessed.csv\")\n",
    "df_gemini['model'] = 'Gemini-2FTE' # gemini-2.0-flash-thinking-exp\n",
    "\n",
    "df_deepseek = pd.read_csv(\"results_data/experimental_design_results_deepseek-chat.csv\")\n",
    "df_deepseek['model'] = 'DeepSeek-V3' # deepseek-chat v3\n",
    "\n",
    "df_gpt = pd.read_csv(\"results_data/experimental_design_results_gpt-4.5-preview-2025-02-27.csv\")\n",
    "df_gpt['model'] = 'OpenAI-GPT-4.5' # gpt-4.5-preview-2025-02-27\n",
    "\n",
    "df_grok = pd.read_csv(\"results_data/experimental_design_results_grok-beta.csv\")\n",
    "df_grok['model'] = 'Grok-Beta' # grok-beta\n",
    "\n",
    "df_sonnet = pd.read_csv(\"results_data/experimental_design_results_claude-3-7-sonnet-20250219.csv\")\n",
    "df_sonnet['model'] = 'Claude-Sonnet-3.7' # claude-3-7-sonnet-20250219\n",
    "\n",
    "# Stack data frames\n",
    "df = pd.concat([df_gemini, df_deepseek, df_gpt, df_grok, df_sonnet], ignore_index=True)\n",
    "\n",
    "# Print\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.915 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.922 seconds.\n",
      "Loading model cost 0.926 seconds.\n",
      "Loading model cost 0.918 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.918 seconds.\n",
      "Loading model cost 0.929 seconds.\n",
      "Loading model cost 0.934 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.959 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.978 seconds.\n",
      "Loading model cost 0.980 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.969 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.978 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.978 seconds.\n",
      "Loading model cost 0.991 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.968 seconds.\n",
      "Loading model cost 0.983 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.968 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.988 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Loading model cost 1.016 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Loading model cost 1.129 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function for Chinese texts using Jieba\n",
    "def tokenize_chinese_jieba(text):\n",
    "    # Use Jieba to cut the Chinese text into words\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "# Function to calculate BLEU (using unigram and bigram)\n",
    "def calculate_bleu(candidate_tokens, reference_tokens):\n",
    "    # We use weights (0.5, 0.5) for unigrams and bigrams\n",
    "    try:\n",
    "        return sentence_bleu([reference_tokens], candidate_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate CHRF (character n-gram F-score)\n",
    "def calculate_chrf(candidate_tokens, reference_tokens):\n",
    "    candidate_str = \"\".join(candidate_tokens)\n",
    "    reference_str = \"\".join(reference_tokens)\n",
    "    # Calculates the ratio between the intersection and the union of the characters of the reference\n",
    "    return len(set(candidate_str) & set(reference_str)) / len(set(reference_str)) if len(set(reference_str)) > 0 else 0\n",
    "\n",
    "# Function to calculate TER using TF-IDF vectorization and mean squared error\n",
    "def calculate_ter(candidate_text, reference_text):\n",
    "    vectorizer = TfidfVectorizer() #token_pattern=r\"(?u)\\b\\w+\\b\" - Removed token pattern because it is not needed for chinese\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([candidate_text, reference_text])\n",
    "        return mean_squared_error(tfidf_matrix[0].toarray(), tfidf_matrix[1].toarray())\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate Semantic Similarity (TF-IDF + cosine)\n",
    "def calculate_semantic_similarity(original, translated):\n",
    "    vectorizer = TfidfVectorizer() #token_pattern=r\"(?u)\\b\\w+\\b\" - Removed token pattern because it is not needed for chinese\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([original, translated])\n",
    "        return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Apply metrics to each row of the DataFrame\n",
    "def calculate_metrics_for_row(row):\n",
    "    original_text = row['abstract']\n",
    "    back_translation = row['EN_ZH']\n",
    "\n",
    "    # Tokenize the texts using Jieba\n",
    "    original_tokens = tokenize_chinese_jieba(original_text)\n",
    "    translated_tokens = tokenize_chinese_jieba(back_translation)\n",
    "\n",
    "    # Calculate the metrics:\n",
    "    bleu_value = calculate_bleu(translated_tokens, original_tokens)\n",
    "    chrf_value = calculate_chrf(translated_tokens, original_tokens)\n",
    "    ter_value = calculate_ter(\"\".join(translated_tokens), \"\".join(original_tokens))\n",
    "    semantic_similarity = calculate_semantic_similarity(original_text, back_translation)\n",
    "\n",
    "    return pd.Series([bleu_value, chrf_value, ter_value, semantic_similarity])\n",
    "\n",
    "def calculate_metrics_for_df(df):\n",
    "    # tqdm.pandas(desc=\"Calculating metrics\") # Removed progress bar\n",
    "    return df.apply(calculate_metrics_for_row, axis=1)\n",
    "\n",
    "def apply_parallel(df, func, n_cores=multiprocessing.cpu_count()):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "df[['BLEU', 'CHRF', 'TER', 'Semantic Similarity']] = apply_parallel(df, calculate_metrics_for_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  model                                              EN_ZH  \\\n",
      "0           Gemini-2FTE  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的联系。接着，探讨了在工程教育专...   \n",
      "1           Gemini-2FTE  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的关系。其次，探讨了在工程教育专...   \n",
      "2           Gemini-2FTE  文章首先阐述工程教育专业认证与应用化学生产实习课程的关系。然后探讨工程教育专业认证背景下应用...   \n",
      "3           Gemini-2FTE  天然产物化学是高等院校药学及相关专业的必修课。课程内容章节丰富且理论性强，学生在学习中面临诸...   \n",
      "4           Gemini-2FTE  天然药物化学是高校药学及相关专业的必修课。该课程内容广泛且理论性强，对学生的学习造成挑战。因...   \n",
      "...                 ...                                                ...   \n",
      "1330  Claude-Sonnet-3.7  化学作为一门自然科学，是人们认识和改造世界的重要方式。纵观历史进程，化学通过无数次实验探索逐...   \n",
      "1331  Claude-Sonnet-3.7  化学作为一门自然科学，是人们认识和改造世界的重要方式。纵观历史进程，化学是通过无数次实验探索...   \n",
      "1332  Claude-Sonnet-3.7  绿色化学分析技术指的是在分析过程中最小化或避免使用有害化学物质，从而实现环境保护和可持续发展...   \n",
      "1333  Claude-Sonnet-3.7  绿色化学分析技术旨在最小化或避免在分析过程中使用有害化学品，从而实现环境保护和可持续发展。文...   \n",
      "1334  Claude-Sonnet-3.7  绿色化学分析技术旨在最小化或避免在分析过程中使用有害化学品，从而实现环境保护和可持续发展。文...   \n",
      "\n",
      "          BLEU      CHRF       TER  Semantic Similarity  \n",
      "0     0.693889  0.909091  0.141732             0.078745  \n",
      "1     0.694830  0.909091  0.121337             0.150640  \n",
      "2     0.698251  0.909091  0.148285             0.184432  \n",
      "3     0.413904  0.890110  0.072720             0.127360  \n",
      "4     0.427470  0.813187  0.069251             0.168983  \n",
      "...        ...       ...       ...                  ...  \n",
      "1330  0.626855  0.767857  0.061089             0.144752  \n",
      "1331  0.641299  0.758929  0.061734             0.104854  \n",
      "1332  0.574339  0.769841  0.062160             0.067593  \n",
      "1333  0.576251  0.801587  0.062160             0.067593  \n",
      "1334  0.556604  0.793651  0.062160             0.067593  \n",
      "\n",
      "[1335 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the results:\n",
    "print(df[['model', 'EN_ZH', 'BLEU', 'CHRF', 'TER', 'Semantic Similarity']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global Descriptive Statistics:\n",
      "|       |        BLEU |         CHRF |          TER |   Semantic Similarity |\n",
      "|:------|------------:|-------------:|-------------:|----------------------:|\n",
      "| count | 1335        | 1335         | 1335         |          1335         |\n",
      "| mean  |    0.569254 |    0.837777  |    0.0816742 |             0.10539   |\n",
      "| std   |    0.112392 |    0.0620577 |    0.101293  |             0.102522  |\n",
      "| min   |    0        |    0         |    0.013339  |             0         |\n",
      "| 25%   |    0.494114 |    0.803214  |    0.053176  |             0.0386913 |\n",
      "| 50%   |    0.575586 |    0.844444  |    0.0692514 |             0.0792653 |\n",
      "| 75%   |    0.64965  |    0.879475  |    0.0869565 |             0.135073  |\n",
      "| max   |    0.930949 |    0.96875   |    1         |             0.691046  |\n",
      "\n",
      "Descriptive Statistics by Model:\n",
      "|                                  |   C l a u d e - S o n n e t - 3 . 7 |   D e e p S e e k - V 3 |   G e m i n i - 2 F T E |   G r o k - B e t a |   O p e n A I - G P T - 4 . 5 |\n",
      "|:---------------------------------|------------------------------------:|------------------------:|------------------------:|--------------------:|------------------------------:|\n",
      "| ('BLEU', 'count')                |                         267         |             267         |             267         |         267         |                   267         |\n",
      "| ('BLEU', 'mean')                 |                           0.593466  |               0.603285  |               0.59971   |           0.52227   |                     0.527541  |\n",
      "| ('BLEU', 'std')                  |                           0.101383  |               0.102282  |               0.113836  |           0.107349  |                     0.107158  |\n",
      "| ('BLEU', 'min')                  |                           0.274146  |               0.276982  |               0         |           0.222731  |                     0.257272  |\n",
      "| ('BLEU', '25%')                  |                           0.528483  |               0.527741  |               0.533788  |           0.450274  |                     0.457766  |\n",
      "| ('BLEU', '50%')                  |                           0.599886  |               0.605879  |               0.608069  |           0.53126   |                     0.531762  |\n",
      "| ('BLEU', '75%')                  |                           0.668906  |               0.678086  |               0.679704  |           0.601176  |                     0.593147  |\n",
      "| ('BLEU', 'max')                  |                           0.827158  |               0.824852  |               0.834775  |           0.790471  |                     0.930949  |\n",
      "| ('CHRF', 'count')                |                         267         |             267         |             267         |         267         |                   267         |\n",
      "| ('CHRF', 'mean')                 |                           0.849358  |               0.84943   |               0.853033  |           0.812656  |                     0.824409  |\n",
      "| ('CHRF', 'std')                  |                           0.0476278 |               0.0514739 |               0.0763403 |           0.0636615 |                     0.0565489 |\n",
      "| ('CHRF', 'min')                  |                           0.689655  |               0.688889  |               0         |           0.623762  |                     0.641509  |\n",
      "| ('CHRF', '25%')                  |                           0.817781  |               0.815459  |               0.826856  |           0.771507  |                     0.784314  |\n",
      "| ('CHRF', '50%')                  |                           0.851852  |               0.846154  |               0.863158  |           0.816794  |                     0.825688  |\n",
      "| ('CHRF', '75%')                  |                           0.882965  |               0.887049  |               0.893204  |           0.861197  |                     0.865277  |\n",
      "| ('CHRF', 'max')                  |                           0.960784  |               0.965517  |               0.96875   |           0.94375   |                     0.945455  |\n",
      "| ('TER', 'count')                 |                         267         |             267         |             267         |         267         |                   267         |\n",
      "| ('TER', 'mean')                  |                           0.0818836 |               0.0808461 |               0.0807685 |           0.0824557 |                     0.0824172 |\n",
      "| ('TER', 'std')                   |                           0.101406  |               0.101555  |               0.101454  |           0.101221  |                     0.101579  |\n",
      "| ('TER', 'min')                   |                           0.0159624 |               0.014156  |               0.013339  |           0.0153177 |                     0.0163406 |\n",
      "| ('TER', '25%')                   |                           0.0540822 |               0.0521687 |               0.0519979 |           0.0535443 |                     0.0538049 |\n",
      "| ('TER', '50%')                   |                           0.0700277 |               0.0679428 |               0.0689655 |           0.07129   |                     0.0692514 |\n",
      "| ('TER', '75%')                   |                           0.0889328 |               0.0869565 |               0.0846942 |           0.0907791 |                     0.0902579 |\n",
      "| ('TER', 'max')                   |                           1         |               1         |               1         |           1         |                     1         |\n",
      "| ('Semantic Similarity', 'count') |                         267         |             267         |             267         |         267         |                   267         |\n",
      "| ('Semantic Similarity', 'mean')  |                           0.117223  |               0.128719  |               0.114355  |           0.078798  |                     0.0878526 |\n",
      "| ('Semantic Similarity', 'std')   |                           0.10384   |               0.118981  |               0.101953  |           0.0787699 |                     0.0968871 |\n",
      "| ('Semantic Similarity', 'min')   |                           0         |               0         |               0         |           0         |                     0         |\n",
      "| ('Semantic Similarity', '25%')   |                           0.0483498 |               0.0532538 |               0.0507053 |           0.026719  |                     0.0266958 |\n",
      "| ('Semantic Similarity', '50%')   |                           0.0887031 |               0.101123  |               0.0887031 |           0.0632448 |                     0.067443  |\n",
      "| ('Semantic Similarity', '75%')   |                           0.168398  |               0.166443  |               0.144647  |           0.109687  |                     0.112516  |\n",
      "| ('Semantic Similarity', 'max')   |                           0.473775  |               0.658562  |               0.573152  |           0.440904  |                     0.691046  |\n"
     ]
    }
   ],
   "source": [
    "# Global Descriptive Statistics\n",
    "global_stats = df[['BLEU', 'CHRF', 'TER', 'Semantic Similarity']].describe()\n",
    "print(\"\\nGlobal Descriptive Statistics:\")\n",
    "print(global_stats.to_markdown())  # Output as markdown for journal\n",
    "\n",
    "# Descriptive Statistics by Model\n",
    "model_stats = df.groupby('model')[['BLEU', 'CHRF', 'TER', 'Semantic Similarity']].describe().transpose()\n",
    "\n",
    "# Flatten the multi-level index for better readability in the table\n",
    "model_stats.columns = [' '.join(col).strip() for col in model_stats.columns.values]\n",
    "\n",
    "print(\"\\nDescriptive Statistics by Model:\")\n",
    "print(model_stats.to_markdown()) # Output as markdown for journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results_metrics/results_metrics.csv\", index=False)\n",
    "df.to_excel(\"results_metrics/results_metrics.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
