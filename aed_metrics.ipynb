{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from difflib import SequenceMatcher\n",
    "import multiprocessing\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 267 entries, 0 to 266\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   abstract    267 non-null    object\n",
      " 1   Repetition  267 non-null    int64 \n",
      " 2   ZH_EN       266 non-null    object\n",
      " 3   EN_ZH       267 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 8.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "df_gemini = pd.read_csv(\"results_data/experimental_design_results_gemini-2.0-flash-thinking-exp_reprocessed.csv\")\n",
    "df = df_gemini\n",
    "\n",
    "# Print the first few rows and info for each DataFrame\n",
    "print(\"Gemini DataFrame:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.877 seconds.\n",
      "Loading model cost 0.878 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.881 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.889 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.892 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.879 seconds.\n",
      "Loading model cost 0.899 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.908 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.933 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.957 seconds.\n",
      "Loading model cost 0.955 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.966 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.983 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.986 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.005 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.985 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/bt-conference/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Loading model cost 1.014 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.027 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.065 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.080 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            abstract  \\\n",
      "0  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...   \n",
      "1  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...   \n",
      "2  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程的关联，然后论述了工程教育专业认证背...   \n",
      "3  “天然药物化学”是高等学校药学及相关专业的必修课程。课程章节内容多、理论性强,学生学习面临较...   \n",
      "4  “天然药物化学”是高等学校药学及相关专业的必修课程。课程章节内容多、理论性强,学生学习面临较...   \n",
      "\n",
      "                                               EN_ZH      BLEU      CHRF  \\\n",
      "0  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的联系。接着，探讨了在工程教育专...  0.693889  0.909091   \n",
      "1  文章首先阐述了工程教育专业认证与应用化学专业生产实习课程之间的关系。其次，探讨了在工程教育专...  0.694830  0.909091   \n",
      "2  文章首先阐述工程教育专业认证与应用化学生产实习课程的关系。然后探讨工程教育专业认证背景下应用...  0.698251  0.909091   \n",
      "3  天然产物化学是高等院校药学及相关专业的必修课。课程内容章节丰富且理论性强，学生在学习中面临诸...  0.413904  0.890110   \n",
      "4  天然药物化学是高校药学及相关专业的必修课。该课程内容广泛且理论性强，对学生的学习造成挑战。因...  0.427470  0.813187   \n",
      "\n",
      "        TER  Semantic Similarity  \n",
      "0  0.141732             0.078745  \n",
      "1  0.121337             0.150640  \n",
      "2  0.148285             0.184432  \n",
      "3  0.072720             0.127360  \n",
      "4  0.069251             0.168983  \n",
      "\n",
      "Mean Metrics:\n",
      "BLEU                   0.599710\n",
      "CHRF                   0.853033\n",
      "TER                    0.080768\n",
      "Semantic Similarity    0.114355\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function for Chinese texts using Jieba\n",
    "def tokenize_chinese_jieba(text):\n",
    "    # Use Jieba to cut the Chinese text into words\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "# Function to calculate BLEU (using unigram and bigram)\n",
    "def calculate_bleu(candidate_tokens, reference_tokens):\n",
    "    # We use weights (0.5, 0.5) for unigrams and bigrams\n",
    "    try:\n",
    "        return sentence_bleu([reference_tokens], candidate_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate CHRF (character n-gram F-score)\n",
    "def calculate_chrf(candidate_tokens, reference_tokens):\n",
    "    candidate_str = \"\".join(candidate_tokens)\n",
    "    reference_str = \"\".join(reference_tokens)\n",
    "    # Calculates the ratio between the intersection and the union of the characters of the reference\n",
    "    return len(set(candidate_str) & set(reference_str)) / len(set(reference_str)) if len(set(reference_str)) > 0 else 0\n",
    "\n",
    "# Function to calculate TER using TF-IDF vectorization and mean squared error\n",
    "def calculate_ter(candidate_text, reference_text):\n",
    "    vectorizer = TfidfVectorizer() #token_pattern=r\"(?u)\\b\\w+\\b\" - Removed token pattern because it is not needed for chinese\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([candidate_text, reference_text])\n",
    "        return mean_squared_error(tfidf_matrix[0].toarray(), tfidf_matrix[1].toarray())\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate Semantic Similarity (TF-IDF + cosine)\n",
    "def calculate_semantic_similarity(original, translated):\n",
    "    vectorizer = TfidfVectorizer() #token_pattern=r\"(?u)\\b\\w+\\b\" - Removed token pattern because it is not needed for chinese\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([original, translated])\n",
    "        return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Apply metrics to each row of the DataFrame\n",
    "def calculate_metrics_for_row(row):\n",
    "    original_text = row['abstract']\n",
    "    back_translation = row['EN_ZH']\n",
    "\n",
    "    # Tokenize the texts using Jieba\n",
    "    original_tokens = tokenize_chinese_jieba(original_text)\n",
    "    translated_tokens = tokenize_chinese_jieba(back_translation)\n",
    "\n",
    "    # Calculate the metrics:\n",
    "    bleu_value = calculate_bleu(translated_tokens, original_tokens)\n",
    "    chrf_value = calculate_chrf(translated_tokens, original_tokens)\n",
    "    ter_value = calculate_ter(\"\".join(translated_tokens), \"\".join(original_tokens))\n",
    "    semantic_similarity = calculate_semantic_similarity(original_text, back_translation)\n",
    "\n",
    "    return pd.Series([bleu_value, chrf_value, ter_value, semantic_similarity])\n",
    "\n",
    "def calculate_metrics_for_df(df):\n",
    "    # tqdm.pandas(desc=\"Calculating metrics\") # Removed progress bar\n",
    "    return df.apply(calculate_metrics_for_row, axis=1)\n",
    "\n",
    "def apply_parallel(df, func, n_cores=multiprocessing.cpu_count()):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "df[['BLEU', 'CHRF', 'TER', 'Semantic Similarity']] = apply_parallel(df, calculate_metrics_for_df)\n",
    "\n",
    "# Display the results:\n",
    "print(df[['abstract', 'EN_ZH', 'BLEU', 'CHRF', 'TER', 'Semantic Similarity']].head())\n",
    "\n",
    "# Calculate and print the mean of the metrics\n",
    "print(\"\\nMean Metrics:\")\n",
    "print(df[['BLEU', 'CHRF', 'TER', 'Semantic Similarity']].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results_metrics/results_metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
