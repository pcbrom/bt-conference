{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import anthropic\n",
    "\n",
    "# Specify the path to your .env file\n",
    "dotenv_path = \"/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "# Access and store the environment variable\n",
    "xai_api_key = os.getenv(\"XAI_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "claude_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Config client\n",
    "client_grok = OpenAI(api_key=xai_api_key, base_url=\"https://api.x.ai/v1\")\n",
    "client_gpt = OpenAI(api_key=openai_api_key)\n",
    "client_gemini = genai.Client(api_key=google_api_key)\n",
    "client_ds = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n",
    "client_claude = anthropic.Anthropic(api_key=claude_api_key)\n",
    "\n",
    "# Model\n",
    "model_grok = 'grok-beta'\n",
    "model_gpt = 'gpt-4.5-preview-2025-02-27'\n",
    "model_gemini = 'gemini-2.0-flash-thinking-exp'\n",
    "model_ds = 'deepseek-chat'\n",
    "model_claude = 'claude-3-7-sonnet-20250219'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctual_text = \"刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在那江畔上，一江水冷月光满城的汪洋，我在时间的树下等了你很久，尘凡儿缠我谤我笑我白了头，你看那天边追逐落日的纸鸢，像一盏回首道别夤夜的风灯，我的心似流沙放逐在车辙旁，他日你若再返必颠沛在世上，若遇那秋夜雨倦鸟也淋淋，那却是花墙下弥留的枯黄，君住在钱塘东，妾在临安北，君去时褐衣红，小奴家腰上黄，寻差了罗盘经，错投在泉亭，奴辗转到杭城，君又生余杭。”\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_and_retranslate(client, model, text):\n",
    "    \"\"\"\n",
    "    Translates the given Chinese text to English and then back to Chinese using the specified client and model.\n",
    "\n",
    "    Args:\n",
    "        client: The client object (OpenAI, genai, or anthropic) to use for translation.\n",
    "        model: The model to use for translation.\n",
    "        text: The Chinese text to translate.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the model name, original text, English translation, and retranslated Chinese text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Translate from ZH to EN\n",
    "    prompt_zh_en = f\"Translate the following Chinese text to English, providing only the translated text without any additional explanations or context: {text}\"\n",
    "    \n",
    "    if isinstance(client, OpenAI):\n",
    "        response_zh_en = client.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": prompt_zh_en}])\n",
    "        generated_text_zh_en = response_zh_en.choices[0].message.content\n",
    "    elif isinstance(client, genai.Client):\n",
    "        response_zh_en = client.models.generate_content(model=model, contents=prompt_zh_en)\n",
    "        generated_text_zh_en = response_zh_en.text\n",
    "    elif isinstance(client, anthropic.Anthropic):\n",
    "        response_zh_en = client.messages.create(model=model, max_tokens=1000, messages=[{\"role\": \"user\", \"content\": prompt_zh_en}])\n",
    "        generated_text_zh_en = response_zh_en.content[0].text\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported client type.\")\n",
    "\n",
    "    # Step 2: Translate back from EN to ZH\n",
    "    prompt_en_zh = f\"Translate the following English text to Chinese, providing only the translated text without any additional explanations or context: {generated_text_zh_en}\"\n",
    "\n",
    "    if isinstance(client, OpenAI):\n",
    "        response_en_zh = client.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": prompt_en_zh}])\n",
    "        generated_text_en_zh = response_en_zh.choices[0].message.content\n",
    "    elif isinstance(client, genai.Client):\n",
    "        response_en_zh = client.models.generate_content(model=model, contents=prompt_en_zh)\n",
    "        generated_text_en_zh = response_en_zh.text\n",
    "    elif isinstance(client, anthropic.Anthropic):\n",
    "        response_en_zh = client.messages.create(model=model, max_tokens=1000, messages=[{\"role\": \"user\", \"content\": prompt_en_zh}])\n",
    "        generated_text_en_zh = response_en_zh.content[0].text\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported client type.\")\n",
    "\n",
    "    data = {\n",
    "        'model': [model],\n",
    "        'original_text': [text],\n",
    "        'zh_en': [generated_text_zh_en],\n",
    "        'en_zh': [generated_text_en_zh]\n",
    "    }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating and retranslating using model: gemini\n",
      "Translation and retranslation complete for model: gemini\n",
      "Translating and retranslating using model: deepseek\n",
      "Translation and retranslation complete for model: deepseek\n",
      "Translating and retranslating using model: claude\n",
      "Translation and retranslation complete for model: claude\n",
      "Translating and retranslating using model: grok\n",
      "Translation and retranslation complete for model: grok\n",
      "Translating and retranslating using model: gpt\n",
      "Translation and retranslation complete for model: gpt\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'gemini': {'model': model_gemini, 'client': client_gemini, 'data': None},\n",
    "    'deepseek': {'model': model_ds, 'client': client_ds, 'data': None},\n",
    "    'claude': {'model': model_claude, 'client': client_claude, 'data': None},\n",
    "    'grok': {'model': model_grok, 'client': client_grok, 'data': None},\n",
    "    'gpt': {'model': model_gpt, 'client': client_gpt, 'data': None}\n",
    "}\n",
    "\n",
    "for model_name, model_data in models.items():\n",
    "    print(f\"Translating and retranslating using model: {model_name}\")\n",
    "    model_data['data'] = translate_and_retranslate(model_data['client'], model_data['model'], punctual_text)\n",
    "    print(f\"Translation and retranslation complete for model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           model  \\\n",
      "0  gemini-2.0-flash-thinking-exp   \n",
      "1                  deepseek-chat   \n",
      "2     claude-3-7-sonnet-20250219   \n",
      "3                      grok-beta   \n",
      "4     gpt-4.5-preview-2025-02-27   \n",
      "\n",
      "                                       original_text  \\\n",
      "0  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "1  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "2  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "3  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "4  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "\n",
      "                                               zh_en  \\\n",
      "0  I am the wandering tear on the rings of time,Y...   \n",
      "1  Daolang, excerpt from \"Flower Demon\": \"I am th...   \n",
      "2  Dao Lang, partial lyrics of \"Flower Demon\":\"I ...   \n",
      "3  Here is the translation:Dao Lang, lyrics from ...   \n",
      "4  Dao Lang, lyrics excerpt from \"Hua Yao\":\"I am ...   \n",
      "\n",
      "                                               en_zh  \n",
      "0  我是时间轮环上漂泊的泪滴，你依然能在风中嗅到胭脂的气味。倘若我将誓言刻在那河岸上，一条冰冷的...  \n",
      "1  刀郎，《花妖》节选： \"我是那年轮上流浪的眼泪， 你仍然能闻到风中的胭脂味。 我若是将诺言刻...  \n",
      "2  刀郎，《花妖》部分歌词：\"我是年轮上漂泊的泪，风中尚可闻你的胭脂。若在河滩上刻下誓约，一条寒...  \n",
      "3  刀郎，《花仙子》歌词：“我是年轮上流浪的眼泪，风中还可以闻到胭脂的香气，如果我在河岸上刻下誓...  \n",
      "4  我是年轮上游走的泪， 却让你嗅到风中胭脂的香味。 若在江边刻下一句誓约， 寒江月冷，淹没几座...  \n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "for model_name, model_data in models.items():\n",
    "    df = pd.DataFrame(model_data['data'])\n",
    "    # Remove newline characters and excess spaces from all string columns\n",
    "    for col in df.select_dtypes(include='object'):\n",
    "        df[col] = df[col].str.replace('\\n', '', regex=False).str.replace(r'\\s{2,}', ' ', regex=True).str.strip()\n",
    "    df_list.append(df)\n",
    "\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function for Chinese texts using Jieba\n",
    "def tokenize_chinese_jieba(text):\n",
    "    # Use Jieba to cut the Chinese text into words\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "# Function to calculate BLEU (using unigram and bigram)\n",
    "def calculate_bleu(candidate_tokens, reference_tokens, weights=(0.5, 0.5, 0.0, 0.0)):\n",
    "    # We use weights (0.5, 0.5) for unigrams and bigrams\n",
    "    try:\n",
    "        return sentence_bleu([reference_tokens], candidate_tokens, weights=weights)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BLEU: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate CHRF (character n-gram F-score)\n",
    "def calculate_chrf(candidate_tokens, reference_tokens):\n",
    "    candidate_str = \"\".join(candidate_tokens)\n",
    "    reference_str = \"\".join(reference_tokens)\n",
    "    # Calculates the ratio between the intersection and the union of the characters of the reference\n",
    "    return len(set(candidate_str) & set(reference_str)) / len(set(reference_str)) if len(set(reference_str)) > 0 else 0\n",
    "\n",
    "# Function to calculate TER using TF-IDF vectorization and mean squared error\n",
    "def calculate_ter(candidate_text, reference_text):\n",
    "    vectorizer = TfidfVectorizer() #token_pattern=r\"(?u)\\b\\w+\\b\" - Removed token pattern because it is not needed for chinese\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([candidate_text, reference_text])\n",
    "        return mean_squared_error(tfidf_matrix[0].toarray(), tfidf_matrix[1].toarray())\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating TER: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate Semantic Similarity (TF-IDF + cosine)\n",
    "def calculate_semantic_similarity(original, translated):\n",
    "    vectorizer = TfidfVectorizer() #token_pattern=r\"(?u)\\b\\w+\\b\" - Removed token pattern because it is not needed for chinese\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([original, translated])\n",
    "        return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating Semantic Similarity: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Apply metrics to each row of the DataFrame\n",
    "def calculate_metrics_for_row(row):\n",
    "    original_text = row['original_text']\n",
    "    en_zh = row['en_zh']\n",
    "\n",
    "    # Tokenize the texts using Jieba\n",
    "    original_tokens = tokenize_chinese_jieba(original_text)\n",
    "    translated_tokens = tokenize_chinese_jieba(en_zh)\n",
    "\n",
    "    # Calculate the metrics:\n",
    "    bleu_value = calculate_bleu(translated_tokens, original_tokens)\n",
    "    chrf_value = calculate_chrf(translated_tokens, original_tokens)\n",
    "    ter_value = calculate_ter(\"\".join(translated_tokens), \"\".join(original_tokens))\n",
    "    semantic_similarity = calculate_semantic_similarity(original_text, en_zh)\n",
    "    bleu_value_uniform = calculate_bleu(translated_tokens, original_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "    return pd.Series([bleu_value, bleu_value_uniform, chrf_value, ter_value, semantic_similarity])\n",
    "\n",
    "def calculate_metrics_for_df(df):\n",
    "    # tqdm.pandas(desc=\"Calculating metrics\") # Removed progress bar\n",
    "    return df.apply(calculate_metrics_for_row, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the calculated metrics to new columns in the original DataFrame\n",
    "metrics_df = calculate_metrics_for_df(final_df)\n",
    "final_df[['BLEU', 'BLEU-Unif', 'CHRF', 'TER', 'Semantic Sim']] = metrics_df.reindex(final_df.index)\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'results_metrics/results_punctual.xlsx' with formatting\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to Excel with good formatting\n",
    "excel_file_path = 'results_metrics/results_punctual.xlsx'\n",
    "final_df.to_excel(excel_file_path, index=False, engine='openpyxl')\n",
    "\n",
    "# Apply formatting to the Excel file\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment, Font, PatternFill\n",
    "\n",
    "# Load the workbook\n",
    "try:\n",
    "    wb = load_workbook(excel_file_path)\n",
    "    ws = wb.active\n",
    "\n",
    "    # Format headers\n",
    "    for cell in ws[1]:\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"D9D9D9\", end_color=\"D9D9D9\", fill_type=\"solid\")\n",
    "        cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)\n",
    "\n",
    "    # Auto-adjust column widths\n",
    "    for column in ws.columns:\n",
    "        max_length = 0\n",
    "        column_letter = column[0].column_letter\n",
    "        for cell in column:\n",
    "            try:\n",
    "                if len(str(cell.value)) > max_length:\n",
    "                    max_length = len(str(cell.value))\n",
    "            except:\n",
    "                pass\n",
    "        adjusted_width = (max_length + 2)\n",
    "        ws.column_dimensions[column_letter].width = adjusted_width\n",
    "\n",
    "    # Save the formatted workbook\n",
    "    wb.save(excel_file_path)\n",
    "    print(f\"Results saved to '{excel_file_path}' with formatting\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{excel_file_path}' was not found.  Please ensure the file path is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during formatting: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           model  \\\n",
      "0  gemini-2.0-flash-thinking-exp   \n",
      "1                  deepseek-chat   \n",
      "2     claude-3-7-sonnet-20250219   \n",
      "3                      grok-beta   \n",
      "4     gpt-4.5-preview-2025-02-27   \n",
      "5                        Google    \n",
      "6                          Baidu   \n",
      "7                          Sougo   \n",
      "\n",
      "                                       original_text  \\\n",
      "0  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "1  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "2  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "3  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "4  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "5  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "6  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "7  刀郎，《花妖》部分歌词：“我是那年轮上流浪的眼泪，你仍然能闻到风中的胭脂味，我若是将诺言刻在...   \n",
      "\n",
      "                                               zh_en  \\\n",
      "0  I am the wandering tear on the rings of time,Y...   \n",
      "1  Daolang, excerpt from \"Flower Demon\": \"I am th...   \n",
      "2  Dao Lang, partial lyrics of \"Flower Demon\":\"I ...   \n",
      "3  Here is the translation:Dao Lang, lyrics from ...   \n",
      "4  Dao Lang, lyrics excerpt from \"Hua Yao\":\"I am ...   \n",
      "5  Dao Lang, part of the lyrics of \"Flower Demon\"...   \n",
      "6  Dao Lang, part of the lyrics of \"Flower Demon\"...   \n",
      "7  Dao Lang, part of the lyrics of \"Flower Demon\"...   \n",
      "\n",
      "                                               en_zh      BLEU  BLEU-Unif  \\\n",
      "0  我是时间轮环上漂泊的泪滴，你依然能在风中嗅到胭脂的气味。倘若我将誓言刻在那河岸上，一条冰冷的...  0.250627   0.054542   \n",
      "1  刀郎，《花妖》节选： \"我是那年轮上流浪的眼泪， 你仍然能闻到风中的胭脂味。 我若是将诺言刻...  0.760207   0.648145   \n",
      "2  刀郎，《花妖》部分歌词：\"我是年轮上漂泊的泪，风中尚可闻你的胭脂。若在河滩上刻下誓约，一条寒...  0.311400   0.144887   \n",
      "3  刀郎，《花仙子》歌词：“我是年轮上流浪的眼泪，风中还可以闻到胭脂的香气，如果我在河岸上刻下誓...  0.331122   0.150128   \n",
      "4  我是年轮上游走的泪， 却让你嗅到风中胭脂的香味。 若在江边刻下一句誓约， 寒江月冷，淹没几座...  0.214141   0.052313   \n",
      "5  刀郎，《花魔》部分歌词：“我是年轮上那一滴游离的泪水，风中你犹能闻到胭脂的味道，若我在河岸上...  0.366413   0.134724   \n",
      "6  道郎，《花妖》歌词的一部分：“我是那一年车轮上徘徊的泪水，你仍然可以闻到风中的胭脂味。如果我...  0.253046   0.104533   \n",
      "7  《花妖》歌词部分刀郎:“我是那年轮里流浪的一滴泪，你还能闻到风里的胭脂香。如果我把我的承诺刻...  0.365007   0.206347   \n",
      "\n",
      "       CHRF       TER  Semantic Sim  \n",
      "0  0.646259  0.039216      0.000000  \n",
      "1  0.918367  0.025307      0.645701  \n",
      "2  0.646259  0.039795      0.064810  \n",
      "3  0.653061  0.039186      0.020347  \n",
      "4  0.530612  0.043478      0.000000  \n",
      "5  0.755102  0.039904      0.042312  \n",
      "6  0.693878  0.038432      0.019987  \n",
      "7  0.591837  0.043364      0.045996  \n",
      "Metrics added and saved to 'results_punctual_metrics_Dao_Lang - metrics.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file_path = 'results_punctual Dao Lang.xlsx'  # Corrected file name\n",
    "try:\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "\n",
    "    # Calculate metrics for the DataFrame\n",
    "    results_df = calculate_metrics_for_df(df)\n",
    "\n",
    "    # Define the desired column names\n",
    "    desired_column_names = ['BLEU', 'BLEU-Unif', 'CHRF', 'TER', 'Semantic Sim']\n",
    "\n",
    "    # Check if the number of columns in results_df matches the number of desired column names\n",
    "    if len(results_df.columns) == len(desired_column_names):\n",
    "        # Rename the columns of results_df\n",
    "        results_df.columns = desired_column_names\n",
    "\n",
    "        # Add the metrics to the original DataFrame\n",
    "        for column in results_df.columns:\n",
    "            df[column] = results_df[column]\n",
    "\n",
    "        # Print the updated DataFrame\n",
    "        print(df)\n",
    "\n",
    "        # Save the updated DataFrame to a new Excel file\n",
    "        results_excel_path = 'results_punctual_metrics_Dao_Lang - metrics.xlsx'\n",
    "        df.to_excel(results_excel_path, index=False)\n",
    "        print(f\"Metrics added and saved to '{results_excel_path}'\")\n",
    "    else:\n",
    "        print(\"Error: The number of columns in the metrics DataFrame does not match the expected number of columns.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{excel_file_path}' was not found. Please ensure the file path is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
